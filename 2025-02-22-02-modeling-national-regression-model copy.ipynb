{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pytensor.sparse as sparse\n",
    "import patsy\n",
    "import arviz as az\n",
    "import boto3\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "TRACE_PATH = DATA_DIR / 'trace-revise.nc'\n",
    "INDEX_COL = \"huc12\"\n",
    "bucket_name = \"duke-research\"\n",
    "bucket_prefix = \"\"\n",
    "file_name_inputs = \"inputs.npz\"\n",
    "file_name_gpkg = \"final.gpkg\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ inputs.npz already exists\n",
      "✓ final.gpkg already exists\n"
     ]
    }
   ],
   "source": [
    "for file_name in [file_name_inputs, file_name_gpkg]:\n",
    "    if os.path.exists(DATA_DIR / file_name):\n",
    "        print(f\"✓ {file_name} already exists\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {file_name} from S3...\")\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket_name, file_name, DATA_DIR/ file_name)\n",
    "    print(f\"✓ Successfully downloaded {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf = gpd.read_file(DATA_DIR /  file_name_gpkg)\n",
    "loaded_data = np.load(DATA_DIR / file_name_inputs)\n",
    "W = scipy.sparse.csr_matrix((loaded_data['W_data'], loaded_data['W_indices'], loaded_data['W_indptr']))\n",
    "X = loaded_data['X']\n",
    "y = loaded_data['y']\n",
    "coords = loaded_data['coords']\n",
    "pretty_predictor_cols = loaded_data['pretty_predictor_cols']\n",
    "\n",
    "W_sym = W + W.T\n",
    "W_sym = (W_sym > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefficients:\n",
      "[-8.94796198 -3.34667055 -2.0773022 ]\n",
      "Largest coefficients:\n",
      "[2.69161593 4.70096494 5.28841285]\n",
      "90% range of coefficients:\n",
      "2.5990572982445936\n",
      "Linear Regression R² on training set: 0.3272\n",
      "Linear Regression R² on test set: 0.3266\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=827)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "# Print smallest / largest coefficients as well as 90% range\n",
    "coefs = lr_model.coef_\n",
    "print(\"Smallest coefficients:\")\n",
    "print(coefs[np.argsort(coefs)[:3]])\n",
    "print(\"Largest coefficients:\")\n",
    "print(coefs[np.argsort(coefs)[-3:]])\n",
    "print(\"90% range of coefficients:\")\n",
    "print(np.percentile(coefs, 95) - np.percentile(coefs, 5))\n",
    "\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Linear Regression R² on training set: {r2_train:.4f}\")\n",
    "print(f\"Linear Regression R² on test set: {r2_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost R² on training set: 0.7104\n",
      "XGBoost R² on test set: 0.6374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=pretty_predictor_cols.tolist())\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=pretty_predictor_cols.tolist())\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 827  \n",
    "}\n",
    "\n",
    "num_boost_round = 100\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "y_pred_xgb_train = xgb_model.predict(dtrain)\n",
    "y_pred_xgb_test = xgb_model.predict(dtest)\n",
    "\n",
    "r2_xgb_train = r2_score(y_train, y_pred_xgb_train)\n",
    "r2_xgb_test = r2_score(y_test, y_pred_xgb_test)\n",
    "\n",
    "print(f\"XGBoost R² on training set: {r2_xgb_train:.4f}\")\n",
    "print(f\"XGBoost R² on test set: {r2_xgb_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to run model inference with 64220 samples and 169 predictors\n",
      "Range of response values is -9.25863265991211 to 8.884825706481934\n",
      "Range of predictor values is 0.0 to 1.0\n"
     ]
    }
   ],
   "source": [
    "USE_GAM          = False\n",
    "RANDOM_SUBSAMPLE = True\n",
    "USE_GP           = False\n",
    "USE_DAG          = False\n",
    "TRACK_MU         = True\n",
    "MULTILEVEL_BETA  = True\n",
    "ADD_INTERCEPT    = True\n",
    "FLOAT_FORMAT = \"float32\"\n",
    "\n",
    "n = len(y)\n",
    "p = X.shape[1]\n",
    "\n",
    "pytensor.config.floatX = FLOAT_FORMAT\n",
    "\n",
    "y = y.astype(FLOAT_FORMAT)\n",
    "X = X.astype(FLOAT_FORMAT)\n",
    "W = W.astype(FLOAT_FORMAT)\n",
    "coords = coords.astype(FLOAT_FORMAT)\n",
    "\n",
    "if ADD_INTERCEPT:\n",
    "\n",
    "    # Check to make sure no constant column is present\n",
    "    if np.all(np.abs(X.mean(axis=0)) < 1e-6):\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1), dtype=FLOAT_FORMAT), X], axis=1)\n",
    "        p = X.shape[1]\n",
    "        pretty_predictor_cols = np.concatenate([[\"intercept\"], pretty_predictor_cols], axis=0)\n",
    "\n",
    "print(f\"Preparing to run model inference with {X.shape[0]} samples and {X.shape[1]} predictors\")\n",
    "print(f\"Range of response values is {y.min()} to {y.max()}\")\n",
    "print(f\"Range of predictor values is {X.min()} to {X.max()}\")\n",
    "\n",
    "if RANDOM_SUBSAMPLE:\n",
    "    is_used = np.random.rand(X.shape[0]) < 0.2\n",
    "    n = is_used.sum()\n",
    "else:\n",
    "    is_used = np.ones(X.shape[0], dtype=bool)\n",
    "    n = len(y)\n",
    "\n",
    "dims = {'predictor': pretty_predictor_cols, \"obs_id\": final_gdf.loc[is_used, INDEX_COL].values}\n",
    "\n",
    "if USE_GAM:\n",
    "    spline_df = 3\n",
    "    X_splines = np.stack([patsy.bs(x, df=spline_df) for x in X.T], axis=-1)\n",
    "    dims['spline_degree']=np.arange(spline_df)\n",
    "\n",
    "\n",
    "# If the multilevel beta option is used, the coefficients\n",
    "# are given a multilevel prior in which the huc12 coefficients are grouped\n",
    "# around a huc8 mean, which is in turn grouped around a huc4 mean\n",
    "# and those are grouped around a huc2 mean. A global mean is also\n",
    "# present at the top level. Each covariate gets a different scale parameter\n",
    "# at each level of the model, the with the scale prior parameter getting\n",
    "# progressively smaller from top to bottom. The integer-coded huc12, huc8, etc\n",
    "# are stored as huc12_int and so on in final_gdf.\n",
    "final_gdf_subset = final_gdf.loc[is_used].copy()\n",
    "with pm.Model(coords=dims) as sparse_graph_model:\n",
    "\n",
    "    X_data = pm.Data('X_data', X[is_used], dims=['obs_id', 'predictor'])\n",
    "\n",
    "    if MULTILEVEL_BETA:\n",
    "        unique_per_level = [1] + [len(final_gdf_subset[f'huc{i}'].unique()) for i in [2, 4, 8]]\n",
    "\n",
    "        n_unique_combined = sum(unique_per_level)\n",
    "        n_levels = len(unique_per_level)\n",
    "\n",
    "        cumulative_unique = np.cumsum(unique_per_level)\n",
    "\n",
    "        # Convention on levels is that 0 is for global mean, 1 is for huc2, 2 is for huc4, and 3 is for huc8.\n",
    "        level_as_int = np.zeros(n_unique_combined, dtype=int)\n",
    "        ptr = 0\n",
    "\n",
    "        for i, n_codes in enumerate(unique_per_level):\n",
    "            level_as_int[ptr:ptr+n_codes] = i \n",
    "            ptr += n_codes\n",
    "\n",
    "        # Make exactly one row is marked as `0`\n",
    "        assert np.sum(level_as_int == 0) == 1\n",
    "\n",
    "        # We use a decreasing sequence of scale parameters\n",
    "        # to encourage greater and greater shrinkage as we go down the levels\n",
    "        beta_scales = pm.HalfNormal('beta_scales', sigma=np.asarray([5.0, 1.0, 0.2, 0.04]), shape = [p, n_levels])\n",
    "        print(f\"Found {n_unique_combined} unique huc2, huc4, huc8 values. Breakdown: {unique_per_level}\")\n",
    "\n",
    "        # Create random variables with N(0,1) prior. These will be rescaled and shifted\n",
    "        # to create the final coefficients later on\n",
    "        z = pm.Normal('z', mu=0, sigma=1, shape = [n_unique_combined, p])\n",
    "        \n",
    "        # Rescale the z by picking off the scale parameters for each level\n",
    "        #[n_unique_combined, p] * [n_unique_combined, p]\n",
    "        z_scaled = z * beta_scales[:, level_as_int].T\n",
    "\n",
    "        # Next, our strategy is to add the 0 level to the 1 level, then add the 1 level to the 2 level, and so on\n",
    "        # until we get to the huc8 level. We do this by iteratively indexing using the codes from final_gdf_subset\n",
    "        # and then summing the results. The final result is a [n, p] array of coefficients\n",
    "        beta = pm.math.zeros((n, p))\n",
    "\n",
    "        \n",
    "        # Add global effect (row 0 of z_scaled)\n",
    "        # Use slicing [0:1, :] to maintain 2D shape for broadcasting safety, although [0, :] usually works\n",
    "        beta += z_scaled[0:1, :]\n",
    "\n",
    "        # Add HUC2 effects (indices 1 to cumulative_unique[1]-1)\n",
    "        start = cumulative_unique[0] # = 1\n",
    "        end = cumulative_unique[1]   # = 1 + num_unique_huc2\n",
    "        z_scaled_block = z_scaled[start:end]\n",
    "        # Ensure codes are numpy array of integers for indexing\n",
    "        codes = final_gdf_subset['huc2'].astype('category').cat.codes.values\n",
    "        beta += z_scaled_block[codes]\n",
    "\n",
    "        # Add HUC4 effects (indices cumulative_unique[1] to cumulative_unique[2]-1)\n",
    "        start = cumulative_unique[1]\n",
    "        end = cumulative_unique[2]\n",
    "        z_scaled_block = z_scaled[start:end]\n",
    "        codes = final_gdf_subset['huc4'].astype('category').cat.codes.values\n",
    "        beta += z_scaled_block[codes]\n",
    "\n",
    "        # Add HUC8 effects (indices cumulative_unique[2] to cumulative_unique[3]-1, which is n_unique_combined-1)\n",
    "        start = cumulative_unique[2]\n",
    "        end = cumulative_unique[3] # This equals n_unique_combined\n",
    "        z_scaled_block = z_scaled[start:end]\n",
    "        codes = final_gdf_subset['huc8'].astype('category').cat.codes.values\n",
    "        beta += z_scaled_block[codes]\n",
    "\n",
    "        beta = pm.Deterministic('beta', beta, dims=['obs_id', 'predictor'])\n",
    "        \n",
    "        mu = pm.math.sum(beta * X_data, axis=1)\n",
    "\n",
    "    else:    \n",
    "        intercept = pm.Normal('intercept', mu=y.mean(), sigma=y.std() * 2)\n",
    "        beta_sd = pm.HalfNormal('beta_sd', sigma=5)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=beta_sd, dims='predictor')\n",
    "        mu = intercept + X[is_used] @ beta\n",
    "\n",
    "    # Spatial random effect using geographic coordinates\n",
    "    if USE_GP:\n",
    "        ell = pm.Beta('ell', alpha=2, beta=2)\n",
    "        eta = pm.HalfNormal('eta', sigma=0.05) # Push it to be closer to zero\n",
    "        cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=2, ls=ell)\n",
    "        gp = pm.gp.HSGP(m=[50, 50], c=1.5, parametrization= \"centered\", cov_func=cov_func)\n",
    "        eps = y[is_used] - mu\n",
    "        f = gp.prior(\"f\", X=coords[is_used], hsgp_coeffs_dims=\"basis_coeffs\", gp_dims=\"obs_id\")\n",
    "        mu += f\n",
    "        \n",
    "       \n",
    "    # Storing `mu` can take a lot of memory; this controls\n",
    "    # whether or not it is stored in the trace\n",
    "    if TRACK_MU:\n",
    "        pm.Deterministic('mu', mu)\n",
    "\n",
    "\n",
    "    # This part creates a likelihood for y ~ N(mu, (I-W)ᵀΩ(I-W)) where W is the adjacency matrix\n",
    "    # and Ω is a diagonal matrix with entries ω\n",
    "    # This is a Gaussian DAG model with a sparse precision matrix\n",
    "    if USE_DAG:\n",
    "        ε = pm.math.constant(y) - mu\n",
    "        ω = pm.HalfNormal('ω', 5) # controls the diagonal entries of the precision matrix\n",
    "        γ = pm.HalfNormal(\"γ\", 5) # controls the off-diagonal entries of the precision matrix\n",
    "        G_pt = sparse.as_sparse_or_tensor_variable(W[is_used][:, is_used])\n",
    "\n",
    "        # Make ε be shape (n,1)\n",
    "        ε_col = ε[:, None]  \n",
    "\n",
    "        # Do all the ops as (n,1) => (n,1)\n",
    "        ε_minus_γG_ε  = ε_col - sparse.structured_dot(G_pt,  γ * ε_col)\n",
    "        ε_minus_γGT_ε = ε_col - sparse.structured_dot(G_pt.T, γ * ε_col)\n",
    "\n",
    "        # Convert back to (n,) before summing\n",
    "        resid1 = ε_minus_γG_ε.ravel()\n",
    "        resid2 = ε_minus_γGT_ε.ravel()\n",
    "\n",
    "        # For quadratic form εᵀ(I-γG)ᵀΩ(I-γG)ε\n",
    "        # This is equivalent to ω * (ε - γGε)(ε - γGε)ᵀ\n",
    "        quadratic_form = ω * pt.sum(resid1 * resid2)\n",
    "            \n",
    "        logdet = n * pt.log(ω)\n",
    "        logp =  -0.5 * (n * pt.log(2 * np.pi) + quadratic_form) + 0.5 * logdet\n",
    "        \n",
    "    else:\n",
    "        sigma = pm.HalfCauchy('sigma', beta=1)\n",
    "        likelihood = pm.Normal('likelihood', mu=mu, sigma=sigma, observed=y[is_used])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logp profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: /mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/pymc/pytensorf.py:947\n",
      "  Time in 1000 calls to Function.__call__: 1.118333e+01s\n",
      "  Time in Function.vm.__call__: 11.131971331778914s (99.541%)\n",
      "  Time in thunks: 11.10888934135437s (99.334%)\n",
      "  Total compilation time: 2.106003e-01s\n",
      "    Number of Apply nodes: 23\n",
      "    PyTensor rewrite time: 1.858543e-01s\n",
      "       PyTensor validate time: 1.829937e-03s\n",
      "    PyTensor Linker time (includes C, CUDA code generation/compiling): 0.02292465406935662s\n",
      "       C-cache preloading 1.470507e-02s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 7.874939e-03s\n",
      "           Node ExpandDims{axis=0}(Composite{...}.0) time 5.694540e-04s\n",
      "           Node Composite{...}(beta_scales_log__, [[5.   1. ... .2  0.04]], [[ 1.60943 ... 21887582]]) time 5.598069e-04s\n",
      "           Node Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}(likelihood{[ 0.526272 ... .7284048 ]}, Sum{axis=1}.0, ExpandDims{axis=0}.0, Log.0, Gt.0) time 5.465770e-04s\n",
      "           Node Transpose{axes=[1, 0]}(Composite{...}.0) time 5.369140e-04s\n",
      "           Node Composite{((i0 + i1 + i2 + i3) * i4)}(Subtensor{:stop}.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, X_data) time 4.224880e-04s\n",
      "\n",
      "Time in all call to pytensor.grad() 5.195185e-01s\n",
      "Time since pytensor import 80691.876s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  40.9%    40.9%       4.547s       1.14e-03s     C     4000       4   pytensor.tensor.math.Sum\n",
      "  38.4%    79.4%       4.269s       4.74e-04s     C     9000       9   pytensor.tensor.elemwise.Elemwise\n",
      "  20.6%    99.9%       2.284s       5.71e-04s     C     4000       4   pytensor.tensor.subtensor.AdvancedSubtensor1\n",
      "   0.0%   100.0%       0.005s       1.22e-06s     C     4000       4   pytensor.tensor.subtensor.Subtensor\n",
      "   0.0%   100.0%       0.003s       1.49e-06s     C     2000       2   pytensor.tensor.elemwise.DimShuffle\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  38.2%    38.2%       4.238s       4.24e-03s     C     1000        1   Sum{axis=1}\n",
      "  35.7%    73.9%       3.970s       3.97e-03s     C     1000        1   Composite{((i0 + i1 + i2 + i3) * i4)}\n",
      "  20.6%    94.5%       2.284s       5.71e-04s     C     4000        4   AdvancedSubtensor1\n",
      "   2.8%    97.2%       0.307s       1.53e-04s     C     2000        2   Sum{axes=None}\n",
      "   1.5%    98.7%       0.164s       1.64e-04s     C     1000        1   Composite{(-0.9189385 + (-0.5 * sqr(i0)))}\n",
      "   0.8%    99.5%       0.094s       9.43e-05s     C     1000        1   Mul\n",
      "   0.2%    99.8%       0.026s       2.60e-05s     C     1000        1   Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}\n",
      "   0.1%    99.9%       0.009s       9.01e-06s     C     1000        1   Composite{...}\n",
      "   0.0%    99.9%       0.003s       2.77e-06s     C     1000        1   Composite{...}\n",
      "   0.0%    99.9%       0.003s       2.63e-06s     C     1000        1   Sum{axes=None}\n",
      "   0.0%    99.9%       0.002s       2.04e-06s     C     1000        1   Subtensor{:stop}\n",
      "   0.0%    99.9%       0.002s       1.77e-06s     C     1000        1   ExpandDims{axis=0}\n",
      "   0.0%    99.9%       0.002s       8.52e-07s     C     2000        2   Subtensor{start:stop}\n",
      "   0.0%   100.0%       0.001s       1.39e-06s     C     1000        1   Gt\n",
      "   0.0%   100.0%       0.001s       1.22e-06s     C     1000        1   Transpose{axes=[1, 0]}\n",
      "   0.0%   100.0%       0.001s       1.14e-06s     C     1000        1   Subtensor{start:}\n",
      "   0.0%   100.0%       0.001s       1.07e-06s     C     1000        1   Log\n",
      "   0.0%   100.0%       0.001s       8.57e-07s     C     1000        1   Composite{(i1 + cast{float64}(i3) + i2 + cast{float64}(i0))}\n",
      "   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  38.2%    38.2%       4.238s       4.24e-03s   1000    19   Sum{axis=1}(Composite{((i0 + i1 + i2 + i3) * i4)}.0)\n",
      "  35.7%    73.9%       3.970s       3.97e-03s   1000    18   Composite{((i0 + i1 + i2 + i3) * i4)}(Subtensor{:stop}.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, X_data)\n",
      "   8.0%    81.9%       0.890s       8.90e-04s   1000    12   AdvancedSubtensor1(Subtensor{start:}.0, [  16   16 ... 1583 1583])\n",
      "   6.6%    88.5%       0.729s       7.29e-04s   1000    14   AdvancedSubtensor1(Subtensor{start:stop}.0, [  4   4 ... 1 183 183])\n",
      "   5.4%    93.9%       0.605s       6.05e-04s   1000    16   AdvancedSubtensor1(Subtensor{start:stop}.0, [ 0  0  0 ... 17 17 17])\n",
      "   2.6%    96.5%       0.290s       2.90e-04s   1000     1   Sum{axes=None}(sigma > 0)\n",
      "   1.5%    98.0%       0.164s       1.64e-04s   1000     0   Composite{(-0.9189385 + (-0.5 * sqr(i0)))}(z)\n",
      "   0.8%    98.8%       0.094s       9.43e-05s   1000    10   Mul(z, AdvancedSubtensor1.0)\n",
      "   0.5%    99.4%       0.060s       6.02e-05s   1000     9   AdvancedSubtensor1(Transpose{axes=[1, 0]}.0, [0 1 1 ... 3 3 3])\n",
      "   0.2%    99.6%       0.026s       2.60e-05s   1000    20   Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}(likelihood{[ 0.526272 ... .7284048 ]}, Sum{axis=1}.0, ExpandDims{axis=0}.0, Log.0, Gt.0)\n",
      "   0.1%    99.8%       0.016s       1.63e-05s   1000    21   Sum{axes=None}(sigma > 0)\n",
      "   0.1%    99.9%       0.009s       9.01e-06s   1000     3   Composite{...}(beta_scales_log__, [[5.   1. ... .2  0.04]], [[ 1.60943 ... 21887582]])\n",
      "   0.0%    99.9%       0.003s       2.77e-06s   1000     2   Composite{...}(sigma_log__)\n",
      "   0.0%    99.9%       0.003s       2.63e-06s   1000     4   Sum{axes=None}(beta_scales_log___logprob)\n",
      "   0.0%    99.9%       0.002s       2.04e-06s   1000    17   Subtensor{:stop}(Mul.0, 1)\n",
      "   0.0%    99.9%       0.002s       1.77e-06s   1000     5   ExpandDims{axis=0}(Composite{...}.0)\n",
      "   0.0%    99.9%       0.001s       1.39e-06s   1000     6   Gt(ExpandDims{axis=0}.0, [0])\n",
      "   0.0%   100.0%       0.001s       1.22e-06s   1000     8   Transpose{axes=[1, 0]}(Composite{...}.0)\n",
      "   0.0%   100.0%       0.001s       1.14e-06s   1000    11   Subtensor{start:}(Mul.0, 211)\n",
      "   0.0%   100.0%       0.001s       1.07e-06s   1000     7   Log(ExpandDims{axis=0}.0)\n",
      "   ... (remaining 3 Apply instances account for 0.02%%(0.00s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try installing amdlibm and set the PyTensor flag lib__amdlibm=True. This speeds up only some Elemwise operation.\n",
      "  - With the default gcc libm, exp in float32 is slower than in float64! Try PyTensor flag floatX=float64, or install amdlibm and set the pytensor flags lib__amdlibm=True\n"
     ]
    }
   ],
   "source": [
    "RUN_PROFILING = False\n",
    "if RUN_PROFILING:\n",
    "    sparse_graph_model.profile(sparse_graph_model.logp()).summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "sample: 100%|██████████| 2000/2000 [7:30:35<00:00, 13.52s/it, 1023 steps of size 8.93e-05. acc. prob=0.80]  \n",
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    }
   ],
   "source": [
    "with sparse_graph_model:\n",
    "    trace = pm.sample(return_inferencedata=False, chains=1, tune=1000, draws=1000, cores=1, nuts_sampler=\"numpyro\", progressbar=True)\n",
    "    az.to_netcdf(trace, TRACE_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR / 'trace-revise.nc'):\n",
    "    print(f\"Uploading trace file to S3...\")\n",
    "    s3 = boto3.client('s3')\n",
    "    TRACE_PATH = str(DATA_DIR / 'trace-revise.nc')\n",
    "    s3.upload_file(TRACE_PATH, bucket_name, 'trace-revise.nc')\n",
    "    print(f\"✓ Successfully uploaded trace file to S3\")\n",
    "else:\n",
    "    print(\"Trace file does not exist, nothing to upload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
