{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pytensor.sparse as sparse\n",
    "import patsy\n",
    "import arviz as az\n",
    "import boto3\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "INDEX_COL = \"huc12\"\n",
    "\n",
    "bucket_name = \"duke-research\"\n",
    "bucket_prefix = \"\"\n",
    "file_name_inputs = \"inputs.npz\"\n",
    "file_name_gpkg = \"final.gpkg\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ inputs.npz already exists\n",
      "✓ final.gpkg already exists\n"
     ]
    }
   ],
   "source": [
    "OVERWRITE = False\n",
    "\n",
    "for file_name in [file_name_inputs, file_name_gpkg]:\n",
    "    if os.path.exists(DATA_DIR / file_name) and not OVERWRITE:\n",
    "        print(f\"✓ {file_name} already exists\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {file_name} from S3...\")\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket_name, file_name, DATA_DIR / file_name)\n",
    "    print(f\"✓ Successfully downloaded {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gdf = gpd.read_file(DATA_DIR /  file_name_gpkg)\n",
    "loaded_data = np.load(DATA_DIR / file_name_inputs)\n",
    "W = scipy.sparse.csr_matrix((loaded_data['W_data'], loaded_data['W_indices'], loaded_data['W_indptr']))\n",
    "X = loaded_data['X']\n",
    "Z = loaded_data['Z']\n",
    "y = loaded_data['y']\n",
    "coords = loaded_data['coords']\n",
    "pretty_predictor_cols = loaded_data['pretty_predictor_cols']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear baseline model (predictors X):\n",
      "Smallest coefficients:\n",
      "[-3.4305146 -2.8717034 -2.8367   ]\n",
      "Largest coefficients:\n",
      "[2.0799415 2.4863663 4.7229114]\n",
      "90% range of coefficients:\n",
      "2.511277484893798\n",
      "Linear Regression R² on training set: 0.3234\n",
      "Linear Regression R² on test set: 0.3254\n",
      "\n",
      "Linear baseline model (embeddings Z):\n",
      "Smallest coefficients:\n",
      "[-1.4075464  -0.47516063 -0.4404625 ]\n",
      "Largest coefficients:\n",
      "[0.36082757 0.46202183 0.661205  ]\n",
      "90% range of coefficients:\n",
      "1.2200746983289719\n",
      "Linear Regression R² on training set: 0.0893\n",
      "Linear Regression R² on test set: 0.0925\n"
     ]
    }
   ],
   "source": [
    "def baseline_lr(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=827)\n",
    "\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = lr_model.predict(X_train)\n",
    "    y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "    # Print smallest / largest coefficients as well as 90% range\n",
    "    coefs = lr_model.coef_\n",
    "    print(\"Smallest coefficients:\")\n",
    "    print(coefs[np.argsort(coefs)[:3]])\n",
    "    print(\"Largest coefficients:\")\n",
    "    print(coefs[np.argsort(coefs)[-3:]])\n",
    "    print(\"90% range of coefficients:\")\n",
    "    print(np.percentile(coefs, 95) - np.percentile(coefs, 5))\n",
    "\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"Linear Regression R² on training set: {r2_train:.4f}\")\n",
    "    print(f\"Linear Regression R² on test set: {r2_test:.4f}\")\n",
    "\n",
    "print(\"Linear baseline model (predictors X):\")\n",
    "baseline_lr(X, y)\n",
    "\n",
    "print(\"\\nLinear baseline model (embeddings Z):\")\n",
    "baseline_lr(Z, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model (predictors X):\n",
      "XGBoost R² on training set: 0.7104\n",
      "XGBoost R² on test set: 0.6374\n",
      "\n",
      "\n",
      "XGBoost baseline model (embeddings Z):\n",
      "XGBoost R² on training set: 0.2644\n",
      "XGBoost R² on test set: 0.1287\n"
     ]
    }
   ],
   "source": [
    "def baseline_xgb(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=827)\n",
    "\n",
    "        # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, )\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 827  \n",
    "    }\n",
    "\n",
    "    num_boost_round = 100\n",
    "    xgb_model = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "    y_pred_xgb_train = xgb_model.predict(dtrain)\n",
    "    y_pred_xgb_test = xgb_model.predict(dtest)\n",
    "\n",
    "    r2_xgb_train = r2_score(y_train, y_pred_xgb_train)\n",
    "    r2_xgb_test = r2_score(y_test, y_pred_xgb_test)\n",
    "\n",
    "    print(f\"XGBoost R² on training set: {r2_xgb_train:.4f}\")\n",
    "    print(f\"XGBoost R² on test set: {r2_xgb_test:.4f}\")\n",
    "\n",
    "print(\"XGBoost baseline model (predictors X):\")\n",
    "baseline_xgb(X, y)\n",
    "print(\"\\nXGBoost baseline model (embeddings Z):\")\n",
    "baseline_xgb(Z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to run model inference with 64220 samples and 169 predictors\n",
      "Range of response values is -9.25863265991211 to 8.884825706481934\n",
      "Range of predictor values is 0.0 to 1.0\n",
      "Found 1730 unique huc2, huc4, huc8 values. Breakdown: [1, 18, 191, 1520]\n"
     ]
    }
   ],
   "source": [
    "USE_GAM          = False\n",
    "RANDOM_SUBSAMPLE = False\n",
    "USE_GP           = True\n",
    "TRACK_MU         = True\n",
    "MULTILEVEL_BETA  = True\n",
    "ADD_INTERCEPT    = False\n",
    "USE_EMBED        = True\n",
    "FLOAT_FORMAT     = \"float32\"\n",
    "HUC_LEVELS       = [2, 4] \n",
    "\n",
    "# HSGP settings; only used if USE_GP is True\n",
    "m = 60\n",
    "c = 1.5\n",
    "\n",
    "n = len(y)\n",
    "p = X.shape[1]\n",
    "\n",
    "pytensor.config.floatX = FLOAT_FORMAT\n",
    "\n",
    "y = y.astype(FLOAT_FORMAT) # Response variable, shape (n,)\n",
    "X = X.astype(FLOAT_FORMAT) # Covariates, shape (n, p)\n",
    "W = W.astype(FLOAT_FORMAT) # Adj matrix (unused, shape (n,n))\n",
    "Z = Z.astype(FLOAT_FORMAT) # Embeddings, shape (n, embed_dim)\n",
    "coords = coords.astype(FLOAT_FORMAT) # locations of rows for spatial random effect (n,2) \n",
    "\n",
    "if ADD_INTERCEPT:\n",
    "\n",
    "    # Check to make sure no constant column is present\n",
    "    if np.all(np.abs(X.mean(axis=0)) < 1e-6):\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1), dtype=FLOAT_FORMAT), X], axis=1)\n",
    "        p = X.shape[1]\n",
    "        pretty_predictor_cols = np.concatenate([[\"intercept\"], pretty_predictor_cols], axis=0)\n",
    "\n",
    "print(f\"Preparing to run model inference with {X.shape[0]} samples and {X.shape[1]} predictors\")\n",
    "print(f\"Range of response values is {y.min()} to {y.max()}\")\n",
    "print(f\"Range of predictor values is {X.min()} to {X.max()}\")\n",
    "\n",
    "if RANDOM_SUBSAMPLE:\n",
    "    is_used = np.random.rand(X.shape[0]) < 0.1\n",
    "    n = is_used.sum()\n",
    "else:\n",
    "    is_used = np.ones(X.shape[0], dtype=bool)\n",
    "    n = len(y)\n",
    "\n",
    "final_gdf_subset = final_gdf.loc[is_used].copy()\n",
    "bigger_hucs = sum([final_gdf_subset[f'huc{i}'].unique().tolist() for i in HUC_LEVELS], [])\n",
    "\n",
    "dims = {\n",
    "    'predictor': pretty_predictor_cols,\n",
    "    \"obs_id\": final_gdf.loc[is_used, INDEX_COL].values,\n",
    "    \"beta_hucs\":  ['global_mean'] + bigger_hucs,\n",
    "\n",
    "}\n",
    "\n",
    "embed_len = Z.shape[1]\n",
    "\n",
    "if USE_GAM:\n",
    "    spline_df = 3\n",
    "    X_splines = np.stack([patsy.bs(x, df=spline_df) for x in X.T], axis=-1)\n",
    "    dims['spline_degree']=np.arange(spline_df)\n",
    "\n",
    "\n",
    "# If the multilevel beta option is used, the coefficients\n",
    "# are given a multilevel prior in which the huc12 coefficients are grouped\n",
    "# around a huc8 mean, which is in turn grouped around a huc4 mean\n",
    "# and those are grouped around a huc2 mean. A global mean is also\n",
    "# present at the top level. Each covariate gets a different scale parameter\n",
    "# at each level of the model, the with the scale prior parameter getting\n",
    "# progressively smaller from top to bottom. The integer-coded huc12, huc8, etc\n",
    "# are stored as huc12_int and so on in final_gdf.\n",
    "with pm.Model(coords=dims) as hierarchical_model:\n",
    "\n",
    "    X_data = pm.Data('X_data', X[is_used], dims=['obs_id', 'predictor'])\n",
    "    Z_data = pm.Data('Z_data', Z[is_used])\n",
    "\n",
    "    if MULTILEVEL_BETA:\n",
    "\n",
    "        # Number of unique HUCs per resolution, starting with the \n",
    "        # global mean which is the same across all HUCs.\n",
    "        unique_per_level = [1] + [len(final_gdf_subset[f'huc{i}'].unique()) for i in HUC_LEVELS]\n",
    "\n",
    "        n_unique_combined = sum(unique_per_level)\n",
    "        n_levels = len(unique_per_level)\n",
    "\n",
    "        cumulative_unique = np.cumsum(unique_per_level)\n",
    "\n",
    "        # Convention on levels is that 0 is for global mean, 1 is for huc2, 2 is for huc4, and 3 is for huc8.\n",
    "        level_as_int = np.zeros(n_unique_combined, dtype=int)\n",
    "        ptr = 0\n",
    "\n",
    "        for i, n_codes in enumerate(unique_per_level):\n",
    "            level_as_int[ptr:ptr+n_codes] = i \n",
    "            ptr += n_codes\n",
    "\n",
    "        # Make sure exactly one row is marked as `0`\n",
    "        assert np.sum(level_as_int == 0) == 1\n",
    "\n",
    "        # We use a decreasing sequence of scale parameters\n",
    "        # to encourage greater and greater shrinkage as we go down the levels\n",
    "        sigma_sequence = pm.math.constant([1.0, 0.2, 0.04], dtype=FLOAT_FORMAT)\n",
    "        sigma_factor = pm.HalfNormal('sigma_factor')\n",
    "        beta_scales = pm.HalfNormal('beta_scales', sigma_sequence * sigma_factor, shape = [p, n_levels])\n",
    "        print(f\"Found {n_unique_combined} unique huc2, huc4 values. Breakdown: {unique_per_level}\")\n",
    "\n",
    "        # Create random variables with N(0,1) prior. These will be rescaled and shifted\n",
    "        # to create the final coefficients later on\n",
    "        u = pm.Normal('u', mu=0, sigma=beta_scales[:, level_as_int].T, dims = [\"beta_hucs\", \"predictor\"])\n",
    "    \n",
    "        # 1. Get integer codes for each level (ensure integer type)\n",
    "        # Use .values to get numpy array, specify dtype for JAX\n",
    "        codes_huc2 = final_gdf_subset['huc2'].astype('category').cat.codes.values.astype(np.int32)\n",
    "        codes_huc4 = final_gdf_subset['huc4'].astype('category').cat.codes.values.astype(np.int32)\n",
    "        #codes_huc8 = final_gdf_subset['huc8'].astype('category').cat.codes.values.astype(np.int32)\n",
    "\n",
    "        # 2. Calculate the indices into the full 'u' tensor for each observation\n",
    "        # Global mean index is 0 for all observations\n",
    "        # HUC2 indices start after global mean (at cumulative_unique[0] = 1)\n",
    "        # HUC4 indices start after HUC2 (at cumulative_unique[1])\n",
    "        # HUC8 indices start after HUC4 (at cumulative_unique[2])\n",
    "        indices_huc2 = codes_huc2 + cumulative_unique[0]\n",
    "        indices_huc4 = codes_huc4 + cumulative_unique[1]\n",
    "        #indices_huc8 = codes_huc8 + cumulative_unique[2]\n",
    "\n",
    "        # 3. Construct beta by indexing 'u' directly and summing components\n",
    "        # Start with global effect (index 0), broadcasted to all n observations\n",
    "        beta_global = pt.zeros([n,p]) # Shape (1, p), will broadcast\n",
    "        beta_global = pt.subtensor.inc_subtensor(beta_global[:], u[0:1, :])\n",
    "        \n",
    "        # Add HUC level effects by indexing the full 'u' tensor\n",
    "        # u[indices_hucX] performs the advanced indexing directly\n",
    "        beta = beta_global + u[indices_huc2] + u[indices_huc4] #+ u[indices_huc8]\n",
    "        # Each u[indices_...] should have shape (n, p)\n",
    "        # Broadcasting of beta_global handles the addition correctly\n",
    "\n",
    "        beta = pm.Deterministic('beta', beta, dims=['obs_id', 'predictor'])\n",
    "        mu = pm.math.sum(beta * X_data, axis=1)\n",
    "\n",
    "\n",
    "    else:    \n",
    "        intercept = pm.Normal('intercept', mu=y.mean(), sigma=y.std() * 2)\n",
    "        beta_sd = pm.HalfNormal('beta_sd', sigma=5)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=beta_sd, dims='predictor')\n",
    "        mu = intercept + X[is_used] @ beta\n",
    "\n",
    "    # Spatial random effect using geographic coordinates\n",
    "    if USE_GP:\n",
    "        ell = pm.Beta('ell', alpha=2, beta=2)\n",
    "        eta = pm.HalfNormal('eta', sigma=0.05) # Push it to be closer to zero\n",
    "        cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=2, ls=ell)\n",
    "        gp = pm.gp.HSGP(m=[m, m], c=c, parametrization= \"non-centered\", cov_func=cov_func)\n",
    "        eps = y[is_used] - mu\n",
    "        f = gp.prior(\"f\", X=coords[is_used], hsgp_coeffs_dims=\"basis_coeffs\", gp_dims=\"obs_id\",dtype=FLOAT_FORMAT)\n",
    "        mu += f.astype(FLOAT_FORMAT)\n",
    "\n",
    "    if USE_EMBED:\n",
    "        embed_scale1 = pm.HalfNormal(\"embed_scale1\", sigma=0.25)\n",
    "        embed_scale2 = pm.HalfNormal(\"embed_scale2\", sigma=0.25)\n",
    "        beta_embed_1 = pm.Normal('beta_embed_1', mu=0, sigma=embed_scale1, shape=(embed_len, 8))\n",
    "        alpha_embed_1 = pm.Normal('alpha_embed_1', mu=0, sigma=embed_scale1, shape=(1, 8))\n",
    "        beta_embed_2 = pm.Normal('beta_embed_2', mu=0, sigma=embed_scale2, shape=(8, 1))\n",
    "\n",
    "        activation_1 = pm.math.tanh(pm.math.dot(Z[is_used], beta_embed_1) + alpha_embed_1)\n",
    "        mu_embed = pm.Deterministic(\"mu_embed\", pm.math.dot(activation_1, beta_embed_2))\n",
    "    else:\n",
    "        mu_embed = 0.\n",
    "\n",
    "    mu += pt.squeeze(mu_embed)\n",
    "       \n",
    "    # Storing `mu` can take a lot of memory; this controls\n",
    "    # whether or not it is stored in the trace\n",
    "    if TRACK_MU:\n",
    "        pm.Deterministic('mu', mu)\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1)\n",
    "    likelihood = pm.Normal('likelihood', mu=mu, sigma=sigma, observed=y[is_used])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logp profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: /mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/pymc/pytensorf.py:947\n",
      "  Time in 1000 calls to Function.__call__: 9.008232e+00s\n",
      "  Time in Function.vm.__call__: 8.936909575131722s (99.208%)\n",
      "  Time in thunks: 8.894704103469849s (98.740%)\n",
      "  Total compilation time: 3.498150e+00s\n",
      "    Number of Apply nodes: 46\n",
      "    PyTensor rewrite time: 3.240435e-01s\n",
      "       PyTensor validate time: 3.487603e-03s\n",
      "    PyTensor Linker time (includes C, CUDA code generation/compiling): 3.171313166851178s\n",
      "       C-cache preloading 1.881592e-02s\n",
      "       Import time 2.044732e-02s\n",
      "       Node make_thunk time 3.151479e+00s\n",
      "           Node Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}(CGemv{no_inplace}.0, Sum{axis=1}.0, ExpandDims{axis=0}.0, Log.0, Gt.0) time 6.401190e-01s\n",
      "           Node Composite{...}(sigma_log__) time 6.377922e-01s\n",
      "           Node Composite{...}(sigma_factor_log__) time 6.294740e-01s\n",
      "           Node Composite{...}(beta_hucs) time 5.977512e-01s\n",
      "           Node MakeVector{dtype='float32'}(sigma_factor_log___logprob, Sum{axes=None}.0, Sum{axes=None}.0, Sum{axes=None}.0, Sum{axes=None}.0, Sum{axes=None}.0, sigma_log___logprob, Sum{axes=None}.0) time 5.677755e-01s\n",
      "\n",
      "Time in all call to pytensor.grad() 2.740751e-01s\n",
      "Time since pytensor import 385.214s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  44.6%    44.6%       3.964s       2.64e-04s     C    15000      15   pytensor.tensor.elemwise.Elemwise\n",
      "  28.0%    72.6%       2.489s       3.11e-04s     C     8000       8   pytensor.tensor.math.Sum\n",
      "  16.0%    88.5%       1.422s       3.56e-04s     C     4000       4   pytensor.tensor.subtensor.AdvancedSubtensor1\n",
      "   4.9%    93.4%       0.435s       4.35e-04s     C     1000       1   pytensor.tensor.basic.Alloc\n",
      "   3.2%    96.7%       0.288s       4.11e-05s     C     7000       7   pytensor.tensor.elemwise.DimShuffle\n",
      "   2.0%    98.7%       0.181s       1.81e-04s     C     1000       1   pytensor.tensor.blas.Dot22\n",
      "   1.0%    99.7%       0.089s       4.47e-05s     C     2000       2   pytensor.tensor.math.All\n",
      "   0.2%    99.9%       0.016s       1.63e-05s     C     1000       1   pytensor.tensor.blas_c.CGemv\n",
      "   0.1%    99.9%       0.005s       1.23e-06s     C     4000       4   pytensor.tensor.subtensor.Subtensor\n",
      "   0.0%   100.0%       0.003s       1.72e-06s     C     2000       2   pytensor.tensor.basic.MakeVector\n",
      "   0.0%   100.0%       0.001s       1.04e-06s     C     1000       1   pytensor.tensor.shape.Reshape\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  26.0%    26.0%       2.310s       2.31e-03s     C     1000        1   Composite{((i0 + i1 + i2 + i3) * i4)}\n",
      "  24.8%    50.8%       2.207s       2.21e-03s     C     1000        1   Sum{axis=1}\n",
      "  16.0%    66.8%       1.422s       3.56e-04s     C     4000        4   AdvancedSubtensor1\n",
      "  14.6%    81.3%       1.296s       1.30e-03s     C     1000        1   Composite{switch(i2, ((-0.9189385 + (-0.5 * sqr((i1 / i0)))) - log(i0)), -inf)}\n",
      "   4.9%    86.2%       0.435s       4.35e-04s     C     1000        1   Alloc\n",
      "   3.2%    89.4%       0.282s       4.03e-05s     C     7000        7   Sum{axes=None}\n",
      "   2.4%    91.8%       0.210s       1.05e-04s     C     2000        2   Gt\n",
      "   2.0%    93.8%       0.181s       1.81e-04s     C     1000        1   Dot22\n",
      "   1.3%    95.1%       0.113s       1.13e-04s     C     1000        1   Composite{tanh((i0 + i1))}\n",
      "   1.2%    96.2%       0.104s       1.04e-04s     C     1000        1   ExpandDims{axis=0}\n",
      "   1.1%    97.3%       0.098s       3.27e-05s     C     3000        3   ExpandDims{axes=[0, 1]}\n",
      "   1.0%    98.3%       0.089s       4.47e-05s     C     2000        2   All{axes=None}\n",
      "   0.9%    99.3%       0.083s       8.28e-05s     C     1000        1   ExpandDims{axis=1}\n",
      "   0.2%    99.5%       0.016s       1.63e-05s     C     1000        1   CGemv{no_inplace}\n",
      "   0.2%    99.6%       0.014s       1.43e-05s     C     1000        1   Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}\n",
      "   0.1%    99.7%       0.008s       8.25e-06s     C     1000        1   Composite{...}\n",
      "   0.0%    99.8%       0.004s       1.18e-06s     C     3000        3   Composite{(-0.9189385 + (-0.5 * sqr(i0)))}\n",
      "   0.0%    99.8%       0.003s       1.14e-06s     C     3000        3   Subtensor{start:stop}\n",
      "   0.0%    99.8%       0.002s       2.47e-06s     C     1000        1   Composite{...}\n",
      "   0.0%    99.8%       0.002s       2.35e-06s     C     1000        1   Composite{...}\n",
      "   ... (remaining 9 Ops account for   0.15%(0.01s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  26.0%    26.0%       2.310s       2.31e-03s   1000    30   Composite{((i0 + i1 + i2 + i3) * i4)}(Reshape{2}.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, AdvancedSubtensor1.0, X_data)\n",
      "  24.8%    50.8%       2.207s       2.21e-03s   1000    32   Sum{axis=1}(Composite{((i0 + i1 + i2 + i3) * i4)}.0)\n",
      "  14.6%    65.4%       1.296s       1.30e-03s   1000    42   Composite{switch(i2, ((-0.9189385 + (-0.5 * sqr((i1 / i0)))) - log(i0)), -inf)}(AdvancedSubtensor1.0, u, ExpandDims{axes=[0, 1]}.0)\n",
      "   5.4%    70.7%       0.476s       4.76e-04s   1000    13   AdvancedSubtensor1(Subtensor{start:stop}.0, [  12   12 ... 1479 1501])\n",
      "   4.9%    75.6%       0.435s       4.35e-04s   1000    25   Alloc(ExpandDims{axis=1}.0, Composite{...}.0, 6521, predictor)\n",
      "   4.9%    80.5%       0.435s       4.35e-04s   1000    15   AdvancedSubtensor1(Subtensor{start:stop}.0, [ 0  0  0 ... 17 17 17])\n",
      "   4.6%    85.1%       0.412s       4.12e-04s   1000    14   AdvancedSubtensor1(Subtensor{start:stop}.0, [  3   3 ... 3 183 184])\n",
      "   3.0%    88.1%       0.265s       2.65e-04s   1000    43   Sum{axes=None}(sigma > 0)\n",
      "   2.3%    90.5%       0.208s       2.08e-04s   1000    39   Gt(AdvancedSubtensor1.0, [[0]])\n",
      "   2.0%    92.5%       0.181s       1.81e-04s   1000    10   Dot22([[-0.27539 ... 01515645]], beta_embed_1)\n",
      "   1.3%    93.8%       0.113s       1.13e-04s   1000    21   Composite{tanh((i0 + i1))}(Dot22.0, alpha_embed_1)\n",
      "   1.2%    94.9%       0.104s       1.04e-04s   1000    12   ExpandDims{axis=0}(Composite{...}.0)\n",
      "   1.1%    96.0%       0.099s       9.85e-05s   1000    38   AdvancedSubtensor1(Transpose{axes=[1, 0]}.0, [0 1 1 ... 3 3 3])\n",
      "   1.0%    97.0%       0.087s       8.73e-05s   1000    40   All{axes=None}(Gt.0)\n",
      "   0.9%    97.9%       0.083s       8.28e-05s   1000    16   ExpandDims{axis=1}(Subtensor{:stop}.0)\n",
      "   0.8%    98.8%       0.071s       7.14e-05s   1000    41   ExpandDims{axes=[0, 1]}(All{axes=None}.0)\n",
      "   0.3%    99.0%       0.025s       2.54e-05s   1000    22   ExpandDims{axes=[0, 1]}(Composite{...}.0)\n",
      "   0.2%    99.2%       0.016s       1.63e-05s   1000    26   CGemv{no_inplace}(likelihood{[ 0.231523 ... .02353187]}, -1.0, Composite{tanh((i0 + i1))}.0, Squeeze{axis=1}.0, 1.0)\n",
      "   0.2%    99.4%       0.014s       1.43e-05s   1000    34   Composite{switch(i4, ((-0.9189385 + (-0.5 * sqr(((i0 - i1) / i2)))) - i3), -inf)}(CGemv{no_inplace}.0, Sum{axis=1}.0, ExpandDims{axis=0}.0, Log.0, Gt.0)\n",
      "   0.1%    99.5%       0.011s       1.07e-05s   1000    37   Sum{axes=None}(sigma > 0)\n",
      "   ... (remaining 26 Apply instances account for 0.50%%(0.04s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try installing amdlibm and set the PyTensor flag lib__amdlibm=True. This speeds up only some Elemwise operation.\n",
      "  - With the default gcc libm, exp in float32 is slower than in float64! Try PyTensor flag floatX=float64, or install amdlibm and set the pytensor flags lib__amdlibm=True\n"
     ]
    }
   ],
   "source": [
    "RUN_PROFILING = True\n",
    "if RUN_PROFILING:\n",
    "    hierarchical_model.profile(hierarchical_model.logp()).summary()\n",
    "\n",
    "RUN_DEBUGGING = False\n",
    "if RUN_DEBUGGING:\n",
    "    print(hierarchical_model.debug(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 5 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_factor, beta_scales, u, beta_embed_1, alpha_embed_1, beta_embed_2, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cf8d58cd5a406c91546f4d49a170d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 5 tune and 5 draw iterations (10 + 10 draws total) took 138 seconds.\n",
      "Chain 0 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 1 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "The number of samples is too small to check convergence reliably.\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE = False\n",
    "\n",
    "if TEST_MODE:\n",
    "    # Set the number of draws to 5 for testing\n",
    "    # and use the numpyro sampler\n",
    "    tune = 2\n",
    "    draws = 2\n",
    "    cores = 2\n",
    "    chains = 2\n",
    "    trace_filename = \"trace-test.nc\"\n",
    "    single_trace_path = \"trace-test-chain{chain}.nc\"\n",
    "else:\n",
    "    tune = 2000\n",
    "    draws = 500\n",
    "    cores = 1\n",
    "    chains = 2\n",
    "    trace_filename = \"trace-revised.nc\"\n",
    "    single_trace_path = \"trace-revised-chain{chain}.nc\"\n",
    "    \n",
    "trace_path = DATA_DIR / trace_filename\n",
    "\n",
    "\n",
    "tps = []\n",
    "for chain in range(chains):\n",
    "    with hierarchical_model:\n",
    "        single_trace = pm.sample(\n",
    "            return_inferencedata=True, \n",
    "            chains=1, \n",
    "            tune=tune, \n",
    "            draws=draws, \n",
    "            cores=cores, \n",
    "            nuts_sampler=\"numpyro\", \n",
    "            progressbar=True,\n",
    "            target_accept=0.85)\n",
    "\n",
    "    tp = single_trace_path.format(chain=chain)\n",
    "    tps += [tp]\n",
    "    \n",
    "    # Delete old trace if it exists\n",
    "    # and make sure hdf5 file is closed\n",
    "    if os.path.exists(tp):\n",
    "        os.remove(tp)\n",
    "    \n",
    "    az.to_netcdf(single_trace, tp)\n",
    "\n",
    "# After sampling complete across all chains, load\n",
    "# chains and concat across chain dimension. This is\n",
    "# to prevent OOM GPU memory issues on a machine with much more RAM.\n",
    "multi_trace = az.concat([az.from_netcdf(tp) for tp in tps], dim=\"chain\")\n",
    "az.to_netcdf(multi_trace, trace_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading trace file to S3...\n",
      "✓ Successfully uploaded trace file to S3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Uploading trace file to S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(trace_path, bucket_name, trace_filename)\n",
    "print(f\"✓ Successfully uploaded trace file to S3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
