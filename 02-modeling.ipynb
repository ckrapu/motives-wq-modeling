{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pytensor.sparse as sparse\n",
    "import patsy\n",
    "import arviz as az\n",
    "import boto3\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "INDEX_COL = \"huc12\"\n",
    "\n",
    "bucket_name = \"duke-research\"\n",
    "bucket_prefix = \"\"\n",
    "file_name_inputs = \"inputs.npz\"\n",
    "file_name_gpkg = \"final.gpkg\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ inputs.npz already exists\n",
      "✓ final.gpkg already exists\n"
     ]
    }
   ],
   "source": [
    "OVERWRITE = True\n",
    "\n",
    "for file_name in [file_name_inputs, file_name_gpkg]:\n",
    "    if os.path.exists(DATA_DIR / file_name) and not OVERWRITE:\n",
    "        print(f\"✓ {file_name} already exists\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {file_name} from S3...\")\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket_name, file_name, DATA_DIR / file_name)\n",
    "    print(f\"✓ Successfully downloaded {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from disk with 64220 rows and 154 columns\n"
     ]
    }
   ],
   "source": [
    "final_gdf = gpd.read_file(DATA_DIR /  file_name_gpkg)\n",
    "loaded_data = np.load(DATA_DIR / file_name_inputs)\n",
    "W = scipy.sparse.csr_matrix((loaded_data['W_data'], loaded_data['W_indices'], loaded_data['W_indptr']))\n",
    "X = loaded_data['X']\n",
    "Z = loaded_data['Z']\n",
    "y = loaded_data['y']\n",
    "coords = loaded_data['coords']\n",
    "pretty_predictor_cols = loaded_data['pretty_predictor_cols']\n",
    "print(f\"Loaded data from disk with {X.shape[0]} rows and {X.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear baseline model (predictors X):\n",
      "Smallest coefficients:\n",
      "[-8.94796198 -3.34667055 -2.0773022 ]\n",
      "Largest coefficients:\n",
      "[2.69161593 4.70096494 5.28841285]\n",
      "90% range of coefficients:\n",
      "2.5990572982445936\n",
      "Linear Regression R² on training set: 0.3272\n",
      "Linear Regression R² on test set: 0.3266\n",
      "\n",
      "Linear baseline model (embeddings Z):\n",
      "Smallest coefficients:\n",
      "[-1.40754657 -0.47516076 -0.4404622 ]\n",
      "Largest coefficients:\n",
      "[0.36082777 0.46202153 0.66120501]\n",
      "90% range of coefficients:\n",
      "1.2200746171583043\n",
      "Linear Regression R² on training set: 0.0893\n",
      "Linear Regression R² on test set: 0.0925\n"
     ]
    }
   ],
   "source": [
    "def baseline_lr(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=827)\n",
    "\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = lr_model.predict(X_train)\n",
    "    y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "    # Print smallest / largest coefficients as well as 90% range\n",
    "    coefs = lr_model.coef_\n",
    "    print(\"Smallest coefficients:\")\n",
    "    print(coefs[np.argsort(coefs)[:3]])\n",
    "    print(\"Largest coefficients:\")\n",
    "    print(coefs[np.argsort(coefs)[-3:]])\n",
    "    print(\"90% range of coefficients:\")\n",
    "    print(np.percentile(coefs, 95) - np.percentile(coefs, 5))\n",
    "\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"Linear Regression R² on training set: {r2_train:.4f}\")\n",
    "    print(f\"Linear Regression R² on test set: {r2_test:.4f}\")\n",
    "\n",
    "print(\"Linear baseline model (predictors X):\")\n",
    "baseline_lr(X, y)\n",
    "\n",
    "print(\"\\nLinear baseline model (embeddings Z):\")\n",
    "baseline_lr(Z, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model (predictors X):\n"
     ]
    }
   ],
   "source": [
    "def baseline_xgb(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=827)\n",
    "\n",
    "        # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, )\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 827  \n",
    "    }\n",
    "\n",
    "    num_boost_round = 100\n",
    "    xgb_model = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "    y_pred_xgb_train = xgb_model.predict(dtrain)\n",
    "    y_pred_xgb_test = xgb_model.predict(dtest)\n",
    "\n",
    "    r2_xgb_train = r2_score(y_train, y_pred_xgb_train)\n",
    "    r2_xgb_test = r2_score(y_test, y_pred_xgb_test)\n",
    "\n",
    "    print(f\"XGBoost R² on training set: {r2_xgb_train:.4f}\")\n",
    "    print(f\"XGBoost R² on test set: {r2_xgb_test:.4f}\")\n",
    "\n",
    "print(\"XGBoost baseline model (predictors X):\")\n",
    "baseline_xgb(X, y)\n",
    "print(\"\\nXGBoost baseline model (embeddings Z):\")\n",
    "baseline_xgb(Z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to run model inference with 64220 samples and 169 predictors\n",
      "Range of response values is -9.25863265991211 to 8.884825706481934\n",
      "Range of predictor values is 0.0 to 1.0\n",
      "Found 209 unique huc2, huc4 values. Breakdown: [1, 18, 190]\n"
     ]
    }
   ],
   "source": [
    "USE_GAM          = False\n",
    "RANDOM_SUBSAMPLE = True\n",
    "USE_GP           = True\n",
    "TRACK_MU         = True\n",
    "MULTILEVEL_BETA  = True\n",
    "ADD_INTERCEPT    = False\n",
    "USE_EMBED        = True\n",
    "FLOAT_FORMAT     = \"float32\"\n",
    "HUC_LEVELS       = [2, 4] \n",
    "\n",
    "# HSGP settings; only used if USE_GP is True\n",
    "m = 60\n",
    "c = 1.5\n",
    "\n",
    "n = len(y)\n",
    "p = X.shape[1]\n",
    "\n",
    "pytensor.config.floatX = FLOAT_FORMAT\n",
    "\n",
    "y = y.astype(FLOAT_FORMAT) # Response variable, shape (n,)\n",
    "X = X.astype(FLOAT_FORMAT) # Covariates, shape (n, p)\n",
    "W = W.astype(FLOAT_FORMAT) # Adj matrix (unused, shape (n,n))\n",
    "Z = Z.astype(FLOAT_FORMAT) # Embeddings, shape (n, embed_dim)\n",
    "coords = coords.astype(FLOAT_FORMAT) # locations of rows for spatial random effect (n,2) \n",
    "\n",
    "if ADD_INTERCEPT:\n",
    "\n",
    "    # Check to make sure no constant column is present\n",
    "    if np.all(np.abs(X.mean(axis=0)) < 1e-6):\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1), dtype=FLOAT_FORMAT), X], axis=1)\n",
    "        p = X.shape[1]\n",
    "        pretty_predictor_cols = np.concatenate([[\"intercept\"], pretty_predictor_cols], axis=0)\n",
    "\n",
    "print(f\"Preparing to run model inference with {X.shape[0]} samples and {X.shape[1]} predictors\")\n",
    "print(f\"Range of response values is {y.min()} to {y.max()}\")\n",
    "print(f\"Range of predictor values is {X.min()} to {X.max()}\")\n",
    "\n",
    "if RANDOM_SUBSAMPLE:\n",
    "    is_used = np.random.rand(X.shape[0]) < 0.1\n",
    "    n = is_used.sum()\n",
    "else:\n",
    "    is_used = np.ones(X.shape[0], dtype=bool)\n",
    "    n = len(y)\n",
    "\n",
    "final_gdf_subset = final_gdf.loc[is_used].copy()\n",
    "bigger_hucs = sum([final_gdf_subset[f'huc{i}'].unique().tolist() for i in HUC_LEVELS], [])\n",
    "\n",
    "dims = {\n",
    "    'predictor': pretty_predictor_cols,\n",
    "    \"obs_id\": final_gdf.loc[is_used, INDEX_COL].values,\n",
    "    \"beta_hucs\":  ['global_mean'] + bigger_hucs,\n",
    "\n",
    "}\n",
    "\n",
    "embed_len = Z.shape[1]\n",
    "\n",
    "if USE_GAM:\n",
    "    spline_df = 3\n",
    "    X_splines = np.stack([patsy.bs(x, df=spline_df) for x in X.T], axis=-1)\n",
    "    dims['spline_degree']=np.arange(spline_df)\n",
    "\n",
    "\n",
    "# If the multilevel beta option is used, the coefficients\n",
    "# are given a multilevel prior in which the huc12 coefficients are grouped\n",
    "# around a huc8 mean, which is in turn grouped around a huc4 mean\n",
    "# and those are grouped around a huc2 mean. A global mean is also\n",
    "# present at the top level. Each covariate gets a different scale parameter\n",
    "# at each level of the model, the with the scale prior parameter getting\n",
    "# progressively smaller from top to bottom. The integer-coded huc12, huc8, etc\n",
    "# are stored as huc12_int and so on in final_gdf.\n",
    "with pm.Model(coords=dims) as hierarchical_model:\n",
    "\n",
    "    X_data = pm.Data('X_data', X[is_used], dims=['obs_id', 'predictor'])\n",
    "    Z_data = pm.Data('Z_data', Z[is_used])\n",
    "\n",
    "    if MULTILEVEL_BETA:\n",
    "\n",
    "        # Number of unique HUCs per resolution, starting with the \n",
    "        # global mean which is the same across all HUCs.\n",
    "        unique_per_level = [1] + [len(final_gdf_subset[f'huc{i}'].unique()) for i in HUC_LEVELS]\n",
    "\n",
    "        n_unique_combined = sum(unique_per_level)\n",
    "        n_levels = len(unique_per_level)\n",
    "\n",
    "        cumulative_unique = np.cumsum(unique_per_level)\n",
    "\n",
    "        # Convention on levels is that 0 is for global mean, 1 is for huc2, 2 is for huc4, and 3 is for huc8.\n",
    "        level_as_int = np.zeros(n_unique_combined, dtype=int)\n",
    "        ptr = 0\n",
    "\n",
    "        for i, n_codes in enumerate(unique_per_level):\n",
    "            level_as_int[ptr:ptr+n_codes] = i \n",
    "            ptr += n_codes\n",
    "\n",
    "        # Make sure exactly one row is marked as `0`\n",
    "        assert np.sum(level_as_int == 0) == 1\n",
    "\n",
    "        # We use a decreasing sequence of scale parameters\n",
    "        # to encourage greater and greater shrinkage as we go down the levels\n",
    "        sigma_sequence = pm.math.constant([1.0, 0.2, 0.04], dtype=FLOAT_FORMAT)\n",
    "        sigma_factor = pm.HalfNormal('sigma_factor')\n",
    "        beta_scales = pm.HalfNormal('beta_scales', sigma_sequence * sigma_factor, shape = [p, n_levels])\n",
    "        print(f\"Found {n_unique_combined} unique huc2, huc4 values. Breakdown: {unique_per_level}\")\n",
    "\n",
    "        # Create random variables with N(0,1) prior. These will be rescaled and shifted\n",
    "        # to create the final coefficients later on\n",
    "        u = pm.Normal('u', mu=0, sigma=beta_scales[:, level_as_int].T, dims = [\"beta_hucs\", \"predictor\"])\n",
    "    \n",
    "        # 1. Get integer codes for each level (ensure integer type)\n",
    "        # Use .values to get numpy array, specify dtype for JAX\n",
    "        codes_huc2 = final_gdf_subset['huc2'].astype('category').cat.codes.values.astype(np.int32)\n",
    "        codes_huc4 = final_gdf_subset['huc4'].astype('category').cat.codes.values.astype(np.int32)\n",
    "        #codes_huc8 = final_gdf_subset['huc8'].astype('category').cat.codes.values.astype(np.int32)\n",
    "\n",
    "        # 2. Calculate the indices into the full 'u' tensor for each observation\n",
    "        # Global mean index is 0 for all observations\n",
    "        # HUC2 indices start after global mean (at cumulative_unique[0] = 1)\n",
    "        # HUC4 indices start after HUC2 (at cumulative_unique[1])\n",
    "        # HUC8 indices start after HUC4 (at cumulative_unique[2])\n",
    "        indices_huc2 = codes_huc2 + cumulative_unique[0]\n",
    "        indices_huc4 = codes_huc4 + cumulative_unique[1]\n",
    "        #indices_huc8 = codes_huc8 + cumulative_unique[2]\n",
    "\n",
    "        # 3. Construct beta by indexing 'u' directly and summing components\n",
    "        # Start with global effect (index 0), broadcasted to all n observations\n",
    "        beta_global = pt.zeros([n,p]) # Shape (1, p), will broadcast\n",
    "        beta_global = pt.subtensor.inc_subtensor(beta_global[:], u[0:1, :])\n",
    "        \n",
    "        # Add HUC level effects by indexing the full 'u' tensor\n",
    "        # u[indices_hucX] performs the advanced indexing directly\n",
    "        beta = beta_global + u[indices_huc2] + u[indices_huc4] #+ u[indices_huc8]\n",
    "        # Each u[indices_...] should have shape (n, p)\n",
    "        # Broadcasting of beta_global handles the addition correctly\n",
    "\n",
    "        beta = pm.Deterministic('beta', beta, dims=['obs_id', 'predictor'])\n",
    "        mu = pm.math.sum(beta * X_data, axis=1)\n",
    "\n",
    "\n",
    "    else:    \n",
    "        intercept = pm.Normal('intercept', mu=y.mean(), sigma=y.std() * 2)\n",
    "        beta_sd = pm.HalfNormal('beta_sd', sigma=5)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=beta_sd, dims='predictor')\n",
    "        mu = intercept + X[is_used] @ beta\n",
    "\n",
    "    # Spatial random effect using geographic coordinates\n",
    "    if USE_GP:\n",
    "        ell = pm.Beta('ell', alpha=2, beta=2)\n",
    "        eta = pm.HalfNormal('eta', sigma=0.05) # Push it to be closer to zero\n",
    "        cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=2, ls=ell)\n",
    "        gp = pm.gp.HSGP(m=[m, m], c=c, parametrization= \"non-centered\", cov_func=cov_func)\n",
    "        eps = y[is_used] - mu\n",
    "        f = gp.prior(\"f\", X=coords[is_used], hsgp_coeffs_dims=\"basis_coeffs\", gp_dims=\"obs_id\",dtype=FLOAT_FORMAT)\n",
    "        mu += f.astype(FLOAT_FORMAT)\n",
    "\n",
    "    if USE_EMBED:\n",
    "        embed_scale1 = pm.HalfNormal(\"embed_scale1\", sigma=0.25)\n",
    "        embed_scale2 = pm.HalfNormal(\"embed_scale2\", sigma=0.25)\n",
    "        beta_embed_1 = pm.Normal('beta_embed_1', mu=0, sigma=embed_scale1, shape=(embed_len, 8))\n",
    "        alpha_embed_1 = pm.Normal('alpha_embed_1', mu=0, sigma=embed_scale1, shape=(1, 8))\n",
    "        beta_embed_2 = pm.Normal('beta_embed_2', mu=0, sigma=embed_scale2, shape=(8, 1))\n",
    "\n",
    "        activation_1 = pm.math.tanh(pm.math.dot(Z[is_used], beta_embed_1) + alpha_embed_1)\n",
    "        mu_embed = pm.Deterministic(\"mu_embed\", pm.math.dot(activation_1, beta_embed_2))\n",
    "    else:\n",
    "        mu_embed = 0.\n",
    "\n",
    "    mu += pt.squeeze(mu_embed)\n",
    "       \n",
    "    # Storing `mu` can take a lot of memory; this controls\n",
    "    # whether or not it is stored in the trace\n",
    "    if TRACK_MU:\n",
    "        pm.Deterministic('mu', mu)\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1)\n",
    "    likelihood = pm.Normal('likelihood', mu=mu, sigma=sigma, observed=y[is_used])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logp profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PROFILING = False\n",
    "if RUN_PROFILING:\n",
    "    hierarchical_model.profile(hierarchical_model.logp()).summary()\n",
    "\n",
    "RUN_DEBUGGING = False\n",
    "if RUN_DEBUGGING:\n",
    "    print(hierarchical_model.debug(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 2 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "sample: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it, 1 steps of size 4.93e-01. acc. prob=0.00]\n",
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of samples is too small to check convergence reliably.\n",
      "Only 2 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "sample: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it, 1 steps of size 4.93e-01. acc. prob=0.00]\n",
      "/mnt/m2ssd/data/Dropbox/research/motives/motives-wq-modeling/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of samples is too small to check convergence reliably.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/trace-test.nc')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_MODE = True\n",
    "\n",
    "if TEST_MODE:\n",
    "    # Set the number of draws to 5 for testing\n",
    "    # and use the numpyro sampler\n",
    "    tune = 2\n",
    "    draws = 2\n",
    "    cores = 2\n",
    "    chains = 2\n",
    "    trace_filename = \"trace-test.nc\"\n",
    "    single_trace_path = \"trace-test-chain{chain}.nc\"\n",
    "else:\n",
    "    tune = 2000\n",
    "    draws = 500\n",
    "    cores = 1\n",
    "    chains = 2\n",
    "    trace_filename = \"trace-revised.nc\"\n",
    "    single_trace_path = \"trace-revised-chain{chain}.nc\"\n",
    "    \n",
    "trace_path = DATA_DIR / trace_filename\n",
    "\n",
    "\n",
    "tps = []\n",
    "for chain in range(chains):\n",
    "    with hierarchical_model:\n",
    "        single_trace = pm.sample(\n",
    "            return_inferencedata=True, \n",
    "            chains=1, \n",
    "            tune=tune, \n",
    "            draws=draws, \n",
    "            cores=cores, \n",
    "            nuts_sampler=\"numpyro\", \n",
    "            progressbar=True,\n",
    "            target_accept=0.85)\n",
    "\n",
    "    tp = DATA_DIR / single_trace_path.format(chain=chain)\n",
    "    tps += [tp]\n",
    "    \n",
    "    # Delete old trace if it exists\n",
    "    # and make sure hdf5 file is closed\n",
    "    if os.path.exists(tp):\n",
    "        os.remove(tp)\n",
    "    \n",
    "    az.to_netcdf(single_trace, tp)\n",
    "\n",
    "# After sampling complete across all chains, load\n",
    "# chains and concat across chain dimension. This is\n",
    "# to prevent OOM GPU memory issues on a machine with much more RAM.\n",
    "multi_trace = az.concat([az.from_netcdf(tp) for tp in tps], dim=\"chain\")\n",
    "az.to_netcdf(multi_trace, trace_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading trace file to S3...\n",
      "✓ Successfully uploaded trace file to S3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Uploading trace file to S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(trace_path, bucket_name, trace_filename)\n",
    "print(f\"✓ Successfully uploaded trace file to S3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
